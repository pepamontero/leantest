\documentclass{article}

% Paquetes usados
\usepackage{graphicx} % Imágenes
\usepackage{amsmath} % Algunos símbolos matemáticos
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array} % Matrices, tablas
\usepackage{xcolor} % Colores de texto
\usepackage{enumitem} % Listas con letras
\usepackage[spanish]{babel}
%LEAN
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{color}
\definecolor{keywordcolor}{rgb}{0.8, 0.1, 0.1}   % red
\definecolor{tacticcolor}{rgb}{0.0, 0.1, 0.9}    % blue
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\definecolor{attributecolor}{rgb}{0.7, 0.1, 0.1} % red
\definecolor{backgroundcolor}{rgb}{0.92, 0.92, 0.92} % light grey

% Distancias entre párrafos, quitar sangrías
\setlength{\parindent}{0pt}
\setlength{\parskip}{.8em}

% Espaciados entre palabras en el justificado.
\sloppy

% Título
\title{Formalización de las matemáticas con Lean. Un caso de estudio: Resultados de Topología General.}
\author{Pepa Montero Jimena}
\date{}

% Inline code
\usepackage{tikz}
\tikzset{%
    baseline,
    inner sep=2pt,
    minimum height=12pt,
    rounded corners=2pt  
}
\newcommand{\code}[1]{\mbox{% added this percent
    \ttfamily
    \tikz \node[anchor=base,fill=backgroundcolor]{#1};% added this percent
}}
\newcommand{\bluecode}[1]{\code{\textcolor{blue}{#1}}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\redcode}[1]{\code{\textcolor{red}{#1}}}

% Lean code
\def\lstlanguagefiles{lstlean.tex}
\lstset{language=lean, backgroundcolor=\color{backgroundcolor}} %default language  

% Spell checker settings
% spell-checker: disable

% Math commands
\newcommand{\nat}{\mathbb{N}}
\newcommand{\rat}{\mathbb{Q}}

% Other commands
\newcommand{\refpep}[1]{(\ref{#1})}
\newcommand{\quotes}[1]{``#1''}

% Theorem environments
\newtheorem{definition}{Definición}[section]
\newtheorem{proposition}{Proposición}[section]


\begin{document}

\maketitle

\section{Introducción}

Introducción al trabajo.

\section{Lean Theorem Prover}

El proyecto Lean fue iniciado en 2012 por Leonardo de Moura, que formaba parte de Microsoft Research Redmond. Es un proyecto activo con una visión a largo plazo, y con miras en el potencial de automatización en las demostraciones matemáticas.\cite{avigad2015theorem}

Lean es un asistente de demostración, es decir, es un software que proporciona un lenguaje para definir objetos, especificar propiedades de esos objetos y probar que estas especificaciones se cumplen. El sistema comprueba que dichas pruebas son correctas y coherentes desde un punto de vista lógico. \cite{leanprover2024} 

Sin embargo, Lean es también un lenguaje de programación, en particular un lenguaje funcional basado en tipos dependientes. Lean 4 es el resultado de volver a implementar el asistente de demostración de Lean 3 en el propio lenguaje de Lean. El nuevo compilador produce código en C, lo que permite a los usuarios implementar automatizaciones de pruebas en Lean de manera eficiente, compilar estas automatizaciones a C y añadirlas como plugins\cite{moura2021lean}.

El objetivo del proyecto Lean es reducir la distancia entre demostraciones matemáticas asistidas y automatizadas, integrando herramientas y métodos automatizados en un marco que facilita la interacción con el usuario y la construcción de pruebas axiomáticas completamente especificadas\cite{avigad2015theorem}.

\subsection{Teoría de tipos dependientes}

[No se hasta qué punto desarrollar este apartado]

Para ambos aspectos de Lean es importante entender la teoría de tipos dependientes en la que se basa. En esta teoría, cada expresión tiene un "tipo" asociado. Por ejemplo, una variable "x" puede hacer referencia a un número natural en un contexto y a un número real en otro.

En Lean podemos declarar variables de un tipo determinado utilizando el operador \code{:}. Por ejemplo \code{n : $\mathbb{N}$} \footnote{Podemos escribir $\mathbb{N}$ en Lean escibiendo \code{$\backslash$nat} y después pulsando espacio.} quiere decir que $n$ es un número natural.

Ejemplos:

\begin{lstlisting}
  variable (n : ℕ)
  variable (f : ℕ → ℕ)
  variable (g : ℕ → ℕ × ℕ)
  variable (p q : ℚ) -- puedo definir varias variables del mismo tipo a la vez
\end{lstlisting}

\code{A → B}

Podemos utilizar entonces el comando \bluecode{$\#$check} para ver que interpreta Lean en cada caso. 

\begin{lstlisting}
  #check n            -- output: n : ℕ
  #check f            -- output: f : ℕ → ℕ
  #check f 2            -- output: f 2 : ℕ
  #check (p, q)            -- output: (p, q) : ℚ × ℚ
\end{lstlisting}

TIPO PROP

Un tipo a destacar en Lean es el tipo \bluecode{Prop}. Un objeto de tipo \bluecode{Prop} es una expresión lógica, que puede tomar los valores verdadero o falso, y se puede operar con los conectores lógicos que conocemos. Podemos declarla como cualquier otra:

\begin{lstlisting}
  variable (P Q : Prop)
  #check P            -- output: P : Prop
  #check ¬P            -- output: ¬P : Prop
  #check P ∧ Q            -- output: P ∧ Q : Prop
\end{lstlisting}

También podemos declarar nuevos tipos y construcciones sobre tipos. Ejemplo:

\begin{lstlisting}
  variable (X : Type)
  variable (F : Type → Type)
  #check F ℕ            -- output: F ℕ : Type
\end{lstlisting}

"Tipo" es un tipo en si mismo.* Jerarquía de tipos. Necesario explicar?

* Tipos dependientes : lista de reales vs. lista de enteros?




\subsection{Lean como asistente de demostración}

Para nuestro fin, nos centraremos en la función de asistente de demostración de Lean. A continuación vamos a introducir las herramientas principales que utilizaremos a la hora de formalizar resultados en Lean 4.



\subsubsection{Definiciones}

Podemos definir constantes en Lean utilizando el comando \bluecode{def} y el operador \code{:=}. Por ejemplo:

\begin{lstlisting}
  def x : ℕ := 2
  def X : Type := ℕ
\end{lstlisting}

Cuando hacemos definiciones también podemos declarar el tipo de objeto que estamos definiendo. Esto no es necesario siempre, porque Lean puede, en general, inferir el tipo directamente.

\begin{lstlisting}
  #check X            -- output: X : Type
  def Y := ℝ
  #check Y            -- output: Y : Type
\end{lstlisting}

\begin{lstlisting}
  def y := 0
  #check y            -- output: y : ℕ
  def z := (0 : ℝ)
  #check z            -- output: z : ℝ
\end{lstlisting}

También podemos definir funciones utilizando el operador \bluecode{fun} (o \bluecode{$\lambda$}), y evaluar estas funciones en elementos concretos utilizando \bluecode{$\#$eval}.

\begin{lstlisting}
  def f : ℕ → ℕ := fun x ↦ x + 5
  def g : ℕ → Prop :=  λ x ↦ x > 0

  #eval f 4            -- output: 9
  #eval f x            -- output: 7
  #eval g 2            -- error: no sabe decidir inmediatamente si 2 > 0
\end{lstlisting}


\subsubsection{Resultados}

Un resultado consiste de varios elementos: el tipo de enunciado (ejemplo, lema o teorema), un nombre, unas hipótesis y una tesis.

Para diferenciar el tipo de enunciado utilizaremos las palabras clave \bluecode{example}, \bluecode{lemma} y \bluecode{theorem} [no se si hay alguno más]. Los resultados de tipo ejemplo no se identifican con un nombre. En el caso de los lemas y los teoremas, el nombre se escribe directamente detrás de la palabra clave.

Las hipótesis, que son opcionales, se escriben después del nombre, utilizando paréntesis para separarlas entre sí. Las hipótesis incluyen objetos que se inicializan (ej. "Sea $n \in \mathbb{N}$" se podría escribir como \code{(n : $\mathbb{N}$)}) y también hipótesis propiamente dichas que suponemos como ciertas. Estas últimas necesitan un nombre seguido de \code{:} (ej. "Supongamos que $n > 0$" se podría escribir como \code{(hn : n > 0)}).

A su vez, utilizamos los dos puntos \code{:} para separarlas de la tesis, que se escribe a continuación.

Al final de la expresión escribimos \code{:=}, y después de este símbolo escribiremos la demostración del enunciado. Por ahora ignoraremos esa parte.

Ejemplos:

\begin{lstlisting}
  example : 2 + 2 = 4 := ...

  lemma my_obvious_lemma (P : Prop) : P → P := ...

  theorem modus_ponens (P Q : Prop) (hP : P) (hPQ : P → Q) : Q := ...

  lemma i_am_error (P Q : Prop) (h : P) : Q := ... 
\end{lstlisting}

En el último ejemplo quería destacar que Lean no pone ninguna pega a un resultado que no sea cierto, este último resultado no dará ningún error (salvo un mensaje de que queda por demostrar). Lean no comprueba la veracidad de los enunciados, sólo comprueba si la demostración a continuación es correcta.

\subsubsection{Mathlib}

[No se donde poner esta sección si antes de defs y resultados o despues o incluso dentro de pruebas??]

La librería de matemáticas de Lean, \textit{Mathlib}, es un proyecto colaborativo con el objetivo de construir una base de datos unificada de definiciones y resultados matemáticos formalizadas en el lenguaje Lean, y cuenta con numerosos colaboradores habituales y actividad diaria\cite{leanprover2024}.

Cómo utilizamos Mathlib?? <- esto es importante

Keywords: exact?, apply?, rw?

Todas sirven para buscar en la librería de mathlib (o en los resultados anteriores que hayamos escrito nosotros) y aplicar la táctica correspondiente.

\subsubsection{Pruebas}

[Muchos ejemplos de esta parte en cierto sentido están sacados del curso de Buzzard y demás. No se como debería escribir esas citas xd]

En Lean, existen dos formas de formalizar demostraciones: utilizando términos y utilizando tácticas. Una prueba es un objeto de tipo \bluecode{Proof}. Un término de tipo \bluecode{Proof} es una representación de una demostración matemática de la veracidad de un enunciado.

Para el objetivo de este trabajo, nos vamos a centrar en las demostraciones que utilizan tácticas. Una demostración de tipo táctico en Lean consiste en una sucesión de comandos o instrucciones, a las que llamamos tácticas, que describen cómo se construye la demostración.

Una vez hemos descrito un resultado o enunciado en Lean, si después de \code{:=} escribimos la palabra clave \bluecode{by}, entraremos en lo que llamamos \textit{modo táctico}. El modo táctico tiene dos particularidades:

Por un lado, contamos en nuestro editor con una nueva ventana llamada \textit{Lean Infoview} que ahora mostrará una lista de elementos. Por ejemplo, si en nuestro archivo lean tenemos lo siguiente

\begin{lstlisting}
  theorem modus_ponens (P Q : Prop) (hP : P) (hPQ : P → Q) : Q := by
\end{lstlisting}

En el infoview ahora veremos lo siguiente:

\begin{lstlisting}
  P Q : Prop
  hP : P
  hPQ : P → Q
  ⊢ Q
\end{lstlisting}

Las tres primeras líneas se corresponden con las hipótesis de nuestro enunciado. La última línea, que siempre comienza con el símbolo \code{$\vdash$}, muestra nuestra tesis, es decir, lo que queremos demostrar en este momento.

Cuando aplicamos una táctica (escribimos un comando) las hipótesis y/o la tesis se actualizan automáticamente en el infoview. Aplicando una táctica detrás de otra, queremos modificar la tesis hasta llegar a algo que es cierto trivialmente. En este caso, el infoview mostrará el mensaje \code{No goals}. Sabremos entonces que hemos completado la prueba correctamente.

En general, y sobretodo en los casos sencillos, existe una relación bastante clara entre estos comandos (tácticas) y las expresiones que utilizamos en las demostraciones formales a las que estamos acostumbrados.

A continuación vamos a ver una a una las tácticas que más se utilizan en las demostraciones.

[NOTA : NO SE SI HACERLO COMO ESTÁ O DEFINIR LAS VARIABLES AL PRINCIPIO NO SE QUE ES MENOS LIOSO]

\begin{itemize}
    \item \textbf{\blue{intro}}
\end{itemize}
    
Imaginemos que queremos demostrar un resultado del estilo de "Si A, entonces B". Normalmente, empezaríamos la demostración diciendo "Supongamos que se da A. Veamos que entonces se tiene B".

En el modo táctico, utilizamos la táctica \bluecode{intro} con esta finalidad. Si en el infoview tenemos

\begin{lstlisting}
  A B : Prop
  ⊢ A → B
\end{lstlisting}

Y escribimos \code{\blue{intro} hA}, donde \code{hA} es el nombre que le queremos dar a la nueva hipótesis, obtendremos

\begin{lstlisting}
  A B : Prop
  hA : A        -- supongamos A
  ⊢ B           -- queremos ver si B
\end{lstlisting}

Por ejemplo, la demostración de nuestro lema \code{my\_obvious\_lemma} debería empezar de la siguiente forma

\begin{lstlisting}
  lemma my_obvious_lemma (P : Prop) : P → P := by
    intro hP
    sorry
\end{lstlisting}

Otra forma de utilizar \bluecode{intro} es en las expresiones de la forma "Para todo $x \in X$, $x$ tiene la propiedad $P$". Normalmente, empezaríamos diciendo "sea un $x \in X$ fijo pero arbitrario... veamos que $x$ satisface $P$".

También utilizaremos \bluecode{intro} en este caso. Si tenemos

\begin{lstlisting}
  P : X → Prop
  ⊢ ∀ (x : X), P x
\end{lstlisting}

y utilizamos \code{\blue{intro} x}, donde \code{x} es la nueva variable que vamos a definir, obtendremos

\begin{lstlisting}
  P : X → Prop
  x : X       -- sea x ∈ X
  ⊢ P x       -- veamos que x cumple P
\end{lstlisting}

\begin{itemize}
  \item \textbf{\textcolor{red}{sorry}}
\end{itemize}

Notemos que en el apartado anterior, la demostración de \code{my\_obvious\_lemma} termina con \redcode{sorry}. Esta es una táctica también, que indica a Lean que la demostración no está terminada, pero que no queremos terminarla por ahora.

Lean mostrará sobre los resultados que acaben en \redcode{sorry} la advertencia \code{declaration uses sorry}. Podemos utilizar resultados incompletos en otras pruebas, pero estas pruebas también estarán marcadas como incompletas.

\begin{itemize}
  \item \textbf{\blue{exact}}
\end{itemize}

Otra táctica fundamental es \bluecode{exact}. Usamos \bluecode{exact} cuando nuestra tesis sea literalmente igual o igual por definición\footnote{Para Lean, literalmente igual es lo mismo que igual por definición.} a una de nuestras hipótesis.

Con esta nueva táctica podemos terminar nuestra demostración de \code{my\_obvious\_lemma}; como en el infoview tenemos, despuñes de utilizar \code{\blue{intro} hP},

\begin{lstlisting}
  P : Prop
  hP : P
  ⊢ P
\end{lstlisting}

Enotnces podemos utilizar \code{\blue{exact} hP} para terminar.

\begin{lstlisting}
  lemma my_obvious_lemma (P : Prop) : P → P := by
    intro hP
    exact hP
\end{lstlisting}

Ahora, en el infoview obtendremos el mensaje \code{No goals}.

Veamos un ejemplo en el que en vez de tener una hipótesis literalmente igual que nuestra tesis, es igual por definición. Consideramos el resultado

\begin{lstlisting}
  example (A B : Set X)
      (x : X) (hx : x ∈ A ∧ x ∈ B) :
      x ∈ A ∩ B := by sorry
\end{lstlisting}

Tenemos un tipo $X$, subconjuntos $A, B \subset X$ y un elemento $x \in X$. Suponemos que $x \in A \land x \in B$ (\code{hx}). Queremos ver que $x \in A \cap B$. Pero, en Lean, $x \in A \cap B$ está definido como $x \in A \land x \in B$, luego realmente tenemos lo mismo. El resultado anterior se puede demostrar, por tanto, utilizando simplemente

\begin{lstlisting}
  example (A B : Set X)
      (x : X) (hx : x ∈ A ∧ x ∈ B) :
      x ∈ A ∩ B := by
    exact hx
\end{lstlisting}



\begin{itemize}
  \item \textbf{\blue{rfl}}
\end{itemize}

Utilizamos \bluecode{rfl} cuando tenemos en el objetivo una expresión con una igualdad, una doble implicación o cualquier otra relación de equivalencia, en la que los términos a ambos lados son exactamente iguales (o iguales por definición). Consiste en aplicar la propiedad reflexiva ($x \cong x$) de las relaciones de equivalencia.

Ejemplos:

\begin{lstlisting}
  example (x : X) : x = x := by
    rfl
\end{lstlisting}

\begin{lstlisting}
  example (A B : Set X) (x : X) :
      (x ∈ A ∪ B) ↔ (x ∈ A ∨ x ∈ B) := by
    rfl
\end{lstlisting}

\begin{itemize}
  \item \textbf{\blue{trivial}}
\end{itemize}

La táctica \bluecode{trivial} se utiliza cuando el objetivo es \code{True} o igual a \code{True} por definición. Ejemplos:

\begin{lstlisting}
  example : True := by
    trivial
\end{lstlisting}

En Lean, existe un subconjunto especial, el \code{Set.univ}, que contiene a todos los elementos del tipo asociado; es el universo, el todo. Por tanto, si $x \in X$, entonces \code{$x \in$ Set.univ} está definido simplemente como \bluecode{True}.

\begin{lstlisting}
  example (x : X) : x ∈ Set.univ := by
    trivial
\end{lstlisting}



\begin{itemize}
  \item \textbf{\textcolor{blue}{apply}}
\end{itemize}

La táctica \bluecode{apply} consiste en aplicar Modus Ponens. Si quiero demostrar B, y tengo como hipótesis "A implica B", entonces basta con demostrar A.

Es decir, si en el infoview tengo algo de la forma

\begin{lstlisting}
  P Q : Prop
  hPQ : P → Q
  ⊢ Q
\end{lstlisting}

y utilizo \code{\blue{apply} hPQ}, paso a tener P como objetivo:

\begin{lstlisting}
  P Q : Prop
  hPQ : P → Q
  ⊢ P
\end{lstlisting}

Con esto, ya podemos demostrar nuestro teorema \code{modus\_ponens}

\begin{lstlisting}
  theorem modus_ponens (P Q : Prop) (hP : P) (hPQ : P → Q) : Q := by
    apply hPQ
    exact hP
\end{lstlisting}

Notar que \bluecode{apply} también sirve para expresiones que aunque no sean implicaciones, son iguales a implicaciones por definición. Por ejemplo, en Lean, $A \subset B$ está definido como $x \in A \implies x \in B$. Por tanto, si tenemos en el infoview

\begin{lstlisting}
  A B : Set X
  x : X
  h : A ⊆ B
  ⊢ x ∈ B
\end{lstlisting}

Podemos utilizar \code{\blue{apply} h}, y la tesis pasará a ser

\begin{lstlisting}
  ⊢ x ∈ A
\end{lstlisting}

También podemos usar apply en otra hipótesis utilizando la palabra clave \blue{at}. Por ejemplo, supongamos que tenemos las hipótesis

\begin{lstlisting}
  hP : P
  hPQ : P → Q
\end{lstlisting}

Aplicar \code{\blue{apply} hPQ \blue{at} hP}, actualizaría las hipótesis a

\begin{lstlisting}
  hP : Q
  hPQ : P → Q
\end{lstlisting}



\begin{itemize}
  \item \textbf{\blue{by\_contra}}
\end{itemize}

Esta táctica es equivalente a utilizar el método de reducción al absurdo. Al aplicarla, añade una nueva hipótesis, resultante de negar la tesis, y la tesis pasa a ser \code{False}.

Por ejemplo, si tenemos en el infoview

\begin{lstlisting}
  ⊢ P
\end{lstlisting}

y aplicamos \code{\blue{by\_contra} h}, ahora tendremos

\begin{lstlisting}
  h : ¬P
  ⊢ False
\end{lstlisting}

Para poder demostrar la tesis \code{False}, necesitamos tener una hipótesis que sea igual o equivalente a \code{False}. También podemos tener dos hipótesis distintas que sean contradictorias (lo que normalmente conocemos como "llegar a una contradicción"). Si tenemos una hipótesis \code{h1}, y otra hipótesis \code{h2} es el resultado de contradecir \code{h1}, entonces podemos utilizar \code{\blue{exact} h2 h1} para demostrar \code{False}.

Podemos demostrar \code{my\_obvious\_lemma} utilizando reducción al absurdo:

\begin{lstlisting}
  lemma my_obvious_lemma (P : Prop) : P → P := by
    intro hP
    by_contra hNP       -- nueva hipotesis hNP : ¬P
    exact hNP hP        -- No Goals
\end{lstlisting}

Notar que si una de mis hipótesis es igual o equivalente a \code{False}, entonces puedo demostrar cualquier cosa. Por ejemplo:

\begin{lstlisting}
  example (h : x ∈ (∅ : Set X)) : 1 = 2 := by
    by_contra         -- no necesitamos la hipotesis 1 ≠ 2
    exact h           -- No Goals
\end{lstlisting}

En Lean, $x \in \emptyset$ está definido simplemente como \code{False}.



\begin{itemize}
  \item \textbf{Resumen de las tácticas básicas}
\end{itemize}

% poner bien el ancho de las filas :/

[Esto igual debería ir en un anexo? no se]

\renewcommand{\arraystretch}{2}

\begin{center}
\begin{tabular}{|  m{8em}  |m{8em} |m{8em}  |} 
  \hline
  \textbf{antes} & \textbf{táctica} & \textbf{después} \\
  \hline
  $\vdash$ P $\rightarrow$ Q & intro hP & \parbox{8em}{hP : P \\ $\vdash$ Q} \\ 
  \hline
  $\vdash \forall$x : X, P x & intro x &  \parbox{8em}{x : X \\ $\vdash$ P x} \\ 
  \hline
  \parbox{8em}{h : P \\ $\vdash$ P}& exact h & No goals\\ 
  \hline
  $\vdash$ x = x & rfl & No goals\\ 
  \hline
  $\vdash$ P $\leftrightarrow$ P & rfl & No goals\\ 
  \hline
  $\vdash$ True & trivial & No goals \\ 
  \hline
  \parbox{8em}{h : P $\rightarrow$ Q \\ $\vdash$ Q} & apply h & $\vdash$ P\\ 
  \hline
  \parbox{8em}{h1 : P $\rightarrow$ Q \\ h2 : P} & apply h1 at h2 & h2 : Q\\ 
  \hline
  $\vdash$ P & by\_contra h & \parbox{8em}{h : $\neg$ P \\ $\vdash$ False} \\\hline
\end{tabular}
\end{center}

\begin{itemize}
  \item \textbf{Utilizar resultados anteriores}
\end{itemize}

Como es habitual en matemáticas a menudo querremos escribir demostraciones que utilicen otros resultados que ya hayamos demostrado previamente. En este caso, podemos aplicar las tácticas anteriores, en particular \bluecode{exact} y \bluecode{apply}, utilizando resultados que ya hemos escrito.

Por ejemplo:

\begin{lstlisting}
  lemma lemma_exact (n : ℕ) : 2^n > 0 := by sorry

  example : 2^5 > 0 := by
    exact lemma_exact 5
\end{lstlisting}

Utilizo \bluecode{exact}, seguido del nombre del resultado que quiero usar y los parámetros que requiere este resultado (en este caso, requiere un número natural). De la misma forma utilizamos apply:

\begin{lstlisting}
  lemma lemma_apply (n : ℕ) : n ≥ 2 → 2^n > n := by sorry

  example : 2^5 > 5 := by
    apply lemma_apply 5        -- la tesis ahora es ⊢ 5 ≥ 2
    sorry
\end{lstlisting}

También podemos hacer referencia a definiciones que hayamos descrito antes

\begin{lstlisting}
  def es_par (n : ℕ) : Prop := ∃ m : ℕ, n = 2 * m

  example : es_par 2 := by sorry
\end{lstlisting}


\begin{itemize}
  \item \textbf{Utilizar los resultados de Mathlib}
\end{itemize}

Además de utilizar los resultados que nosotros hayamos probado antes, es especialmente interesante poder utilizar los resultados que ya están demostrados en la librería de matemáticas de Lean, Mathlib. Sin embargo, encontrar el nombre del resultado que necesitamos en cada momento puede ser complicado.

Contamos con una serie de tácticas que nos facilitan esta tarea. La táctica que más utilizaremos es \bluecode{exact?}. Al escribir \bluecode{exact?} en una demostración, Lean buscará en la librería un resultado que, al aplicar \code{\blue{exact} ese\_resultado}, concluya la prueba. Nos devolverá en el infoview una o varias opciones que podemos probar.

Por ejemplo,

\begin{lstlisting}
  lemma lemma_exact (n : ℕ) : 2^n > 0 := by
    exact?
\end{lstlisting}

nos devuelve en el infoview el mensaje: \code{Try this: \blue{exact Nat.two\_pow\_pos n}}. Podemos simplemente pinchar en la sugerencia y la escribirá sustituyeno a \bluecode{exact?}. Esto completará la prueba, pues \code{Nat.two\_pow\_pos} es un resultado de Mathlib igual o equivalente a nuestro lema. En efecto, si escribimos

\begin{lstlisting}
  #check Nat.two_pow_pos
\end{lstlisting}

Obtenemos el mensaje

\begin{lstlisting}
  Nat.two_pow_pos (w : ℕ) : 0 < 2 ^ w
\end{lstlisting}

La táctica \bluecode{exact?} también busca resultados que nosotros hayamos probado, así que también podemos utilizarlo cuando sepamos que algo ya lo hemos demostrado.

La táctica \bluecode{apply?} funciona de la misma forma, aunque suele sugerir una gran cantidad de resultados distintos y, a menudo, muchos no son realmente útiles.


[No se si poner algo de que a veces no funciona exact? pero podemos intentar como escribir lo que estamos buscando asi generalmente en un ejemplo aparte y ya ver si lo puedes aplicar en tu demo (es como una cosa que hago todo el rato)]


\begin{itemize}
  \item \textbf{Simplificar expresiones}
\end{itemize}


Varias tácticas nos pueden servir para simplificar expresiones. Por un lado tenemos \bluecode{rw} (rewrite). Esta táctica toma entre corchetes una expresión (una hipótesis o un resultado) que sea una equivalencia (como $x = y$ o $P \leftrightarrow Q$) e intenta reescribir cada instancia en la tesis de la parte izquierda de la equivalencia por la parte derecha. Una definición también se considera una equivalencia.

Por ejemplo, el siguiente resultado

\begin{lstlisting}
  example (a b : ℕ) (h : a = b) : es_par (a + b) := by sorry
\end{lstlisting}

imprime en el infoview

\begin{lstlisting}
  a b : ℕ
  h : a = b
  ⊢ es_par (a + b)
\end{lstlisting}

Si ahora escribimos \code{\blue{rw} [h]}, entonces la tesis pasará a ser

\begin{lstlisting}
  ⊢ es_par (b + b)
\end{lstlisting}

Ahora podemos escribir \code{\blue{rw} [es\_par]}, y la tesis cambiará a

\begin{lstlisting}
  ⊢ ∃ m, b + b = 2 * m
\end{lstlisting}

También podemos utilizar \code{\blue{rw} [ · ] \blue{at} h} para aplicar los cambios a una hipótesis \code{h} en lugar de la tesis, y \code{\blue{rw} [ · ] \blue{at} *} para aplicarlos en todas las expresiones posibles.

Para hacer el camino inverso, cambiar las instancias de la parte derecha de la equivalencia a la parte izquierda, utilizamos la flecha hacia la izquierda: \code{\blue{rw} [$\leftarrow$  · ]}.

Una táctica bastante más simple que esta es \bluecode{simp}, que intenta simplificar la tesis utilizando todos los resultados y definiciones posibles de Mathlib (que estén marcados como simplificaciones) y reescribiendo la expresión. 

Existen otras tácticas más particulares, como \bluecode{dsimp}, que funciona como \bluecode{simp} pero sólo utiliza definiciones (y no resultados), o \bluecode{ring}, que utiliza resultados relacionados con anillos para simplificar expresiones.

En el fondo, estas últimas tácticas hacen lo mismo que \bluecode{rw}, pero sin necesitar el paso de buscar en la librería.

\begin{itemize}
  \item \textbf{Conectores lógicos}
\end{itemize}

Algunas de nuestras hipótesis o tesis tendrán conectores lógicos más complicados como $\land$, $\lor$ y $\leftrightarrow$. Esto significará que tendremos que dividir la demostraciones en varias tesis más simples (por ejemplo, si quiero demostrar que se dan A y B, primero demuestro A y luego B).

Cuando trabajemos con varias tesis a la vez, podremos ver arriba del InfoView el número de tesis activas que tenemos, y a continuación veremos la primera tesis, seguida de la siguiente.

Para poder trabajar sólo con una tesis a la vez, por claridad, podemos separar las tesis utilizando puntos \code{·}. Cada punto simboliza una hipótesis distinta.

A continuación vamos a ver varios ejemplos en los que necesitamos trabajar con varias hipótesis, en particular cuando trabajemos con los conectores lógicos $\land$, $\lor$ y $\leftrightarrow$


\begin{enumerate}[label=\alph*., left=35pt]
  \item \textbf{$\lor$ en la tesis: \blue{left}, \blue{right}}
\end{enumerate}

El caso más sencillo es aquel en el que la tesis es de la forma "A o B", y sabemos que se tiene A (o B). En ese caso, utilizaremos la táctica \bluecode{left} (respectivamente \bluecode{right}) para simplificar la tesis.

Por ejemplo, si tenemos en el infoview

\begin{lstlisting}
  P Q : Prop
  h : P
  ⊢ P ∨ Q
\end{lstlisting}

y escribimos

\begin{lstlisting}
  example (P Q : Prop) (h : P) : P ∨ Q := by
    left
\end{lstlisting}

ahora tendremos

\begin{lstlisting}
  P Q : Prop
  h : P
  ⊢ P
\end{lstlisting}

Además, en hipótesis de la forma "A y B", podemos querer utilizar sólo A (o sólo B). Podemos utilizar \code{h.left} (respectivamente \code{h.right}). Por ejemplo

\begin{lstlisting}
  example (P Q : Prop) (h : P ∧ Q) : Q := by
    exact h.right
\end{lstlisting}



\begin{enumerate}[resume, label=\alph*., left=35pt]
  \item \textbf{$\lor$ en las hipótesis: \blue{cases'}}
\end{enumerate}

La táctica \bluecode{cases'}

La táctica "cases'"\footnote{existe una táctica cases pero es menos potente que esta, y esta es la que utilizaremos normalmente} se utiliza sobre una hipótesis de la forma "P $\lor$ Q", y queremos proceder de forma distinta en caso de que P y de que Q.

Es el equivalente a dividir una demostración en casos en el lenguaje natural. Movida: esto genera varios goals. Esto habría que explicarlo bien. Poner que gestionamos los distintos goals con · ?

Esta táctica también tiene otro uso práctico interesante para $\land$ : si se aplica a una hipótesis de la forma "P $\land$ Q", dividimos esta hipótesis en dos. Esto nos puede servir en ocasiones para simplificar las hipótesis y trabajar más fácilmente con ellas.

\begin{itemize}
    \item \textbf{constructor}
\end{itemize}

Podemos utilizar costructor cuando tengamos tesis que se puedan separar en dos partes, por ejemplo tesis de la forma "P $\land$ Q" se convierten en dos tesis distintas, por un lado P y por otro Q. Igualmente, tesis de la forma "P $\leftrightarrow$ Q" se divide en "P $\rightarrow$ Q" y "Q $\rightarrow$ P".


\begin{center}
\begin{tabular}{ | m{8em} | m{8em}| m{8em} | } 
  \hline
  \textbf{antes} & \textbf{táctica} & \textbf{después} \\
  \hline
  $\vdash$ P $\lor$ Q & left & $\vdash$ P \\
  \hline
  $\vdash$ P $\lor$ Q & right & $\vdash$ Q \\
  \hline
  \parbox{8em}{h : P $\lor$ Q \\ $\vdash$ goal} & cases' h with hP hQ & \parbox{8em}{goal 1: \\ hP : P \\ $\vdash$ goal \\[1ex] goal 2: \\ $~~$ hQ : Q \\ $~~$ $\vdash$ goal} \\
  \hline
  h : P $\land$ Q & cases' h with hP hQ & \parbox{8em}{hP : P \\ hQ : Q }\\
  \hline
  
\end{tabular}
\end{center}

\section{Espacios topológicos en Lean}

Explicar algunos ejemplos de definiciones y demostraciones, no a modo de explicación completa de los prerrequisitos de topoogía sino para tener un primer acercamiento sencillo a la topología en Lean.

\subsection{Espacios topolǵicos normales}

Introducción : ¿por qué son interesantes?

\begin{definition}
  Sea $X$ un espacio topológico. $X$ es \emph{normal} si para cada par de cerrados disjuntos $C, D \subseteq X$ existen abiertos disjuntos $U$ y $V$ en $X$ tales  que separan $C$ y $D$, es decir, $C \subseteq U$ y $D \subseteq V$ \textnormal{(véase \cite[p. 99]{willard2012general})}.
\end{definition}

En Lean, escribimos esta definición de la siguiente forma.

\begin{lstlisting}
  def NormalTopoSpace {X : Type} (T : TopologicalSpace X) : Prop :=
    ∀ C : Set X, ∀ D : Set X,
    IsClosed C → IsClosed D → C ∩ D = ∅ →
    ∃ U : Set X, ∃ V : Set X,
      IsOpen U ∧
      IsOpen V ∧
      C ⊆ U ∧
      D ⊆ V ∧
      U ∩ V = ∅
\end{lstlisting}

Ahora queremos dar una caracterización para este tipo de espacios, que nos facilitará el trabajo más adelante.

\begin{proposition}[Caracterización de la normalidad]
  Sea $X$ un espacio topológico. $X$ es normal si y sólo si para cada abierto $U$ y cada cerrado $C$ de $X$ tales que $C \subseteq U$, existe un abierto $V \subset X$ de forma que $C \subseteq V \subseteq \overline{V} \subseteq U$.
\end{proposition}

En Lean, escribimos:

\begin{lstlisting}
  lemma characterization_of_normal {X : Type}
    (T : TopologicalSpace X) :
    NormalTopoSpace T ↔
      ∀ U : Set X, ∀ C : Set X, IsOpen U → IsClosed C → C ⊆ U →
      ∃ V : Set X, IsOpen V ∧ C ⊆ V ∧ (Closure V) ⊆ U := by sorry
\end{lstlisting}

\begin{proof}
  Veamos primero una implicación, y luego la otra (utilizamos \bluecode{constructor}).

  ($\implies$) Supongamos que $X$ es un espacio normal (\code{hT}) y sean $U$ un abierto (\code{hU}) y $C$ un cerrado (\code{hC}) tales que $C \subseteq U$ (\code{hCU}).

\begin{lstlisting}
    intro hT U C hU hC hCU
\end{lstlisting}
  
  Puesto que $X$ es normal, por la definición, para $C$ y $U^c$ cerrados en $X$ obtenemos $V_1$ y $V_2$ abiertos (\code{V1\_open}, \code{V2\_open}) disjuntos (\code{hV}) tales que $C \subseteq V_1$ (\code{hCV}) y $U^c \subseteq V_2$ (\code{hUV}).

\begin{lstlisting}
  obtain ⟨V1, V2, V1_open, V2_open, hCV, hUV, hV⟩ :=
    hT C Uᶜ
    hC
    (by exact isClosed_compl_iff.mpr hU)
    (by rw [ABdisjoint_iff_AsubsBc, compl_compl]; exact hCU)
\end{lstlisting}

  Por supuesto, en Lean tenemos que especificar por qué $U^c$ es cerrado y por qué $U^c \subseteq V_2$. Ahora tenemos una hipótesis de la forma

\begin{lstlisting}
  h : IsOpen V1 ∧ IsOpen V2 ∧ C ⊆ V1 ∧ Uᶜ ⊆ V2 ∧ V1 ∩ V2 = ∅
\end{lstlisting}
  
  Tomamos como $V$ el $V_1$ obtenido de esta forma,

\begin{lstlisting}
    use V1
\end{lstlisting}

  Queremos ver que satisface las condiciones que le pedimos:

\begin{lstlisting}
  ⊢ IsOpen V1 ∧ C ⊆ V1 ∧ Closure V1 ⊆ U
\end{lstlisting}
  
  Cómo tiene que cumplir tres condiciones, tendremos que utilizar \bluecode{constructor} varias veces. En primer lugar, $V_1$ es abierto por construcción. Además, $C \subseteq V_1$ también por construcción.

\begin{lstlisting}
    constructor
    · exact V1_open
    constructor
    · exact hCV
\end{lstlisting}

  Ahora queda demostrar que $\overline{V_1} \subseteq U$. Por un lado, tenemos que $V_1$ y $V_2$ son disjuntos, luego, en particular, como $V_2$ es abierto, se tiene que $\overline{V_1}$ y $V_2$ son disjuntos.

\begin{lstlisting}
    · apply disjointU_V_then_disjointClosureU_V V2_open at hV
      apply Set.disjoint_iff_inter_eq_empty.mpr at hV -- usamos la propiedad Disjoint de Lean
\end{lstlisting}

  Por otro lado, tenemos que $\overline{V_1} \subseteq U$ $\iff$ $V_1$ y $U^c$ son disjuntos. Basta ver que lo son utilizando lo anterior, sabiendo que $U^c \subseteq V_2$.

\begin{lstlisting}
      apply Set.disjoint_compl_right_iff_subset.mp
      exact Set.disjoint_of_subset_right hUV hV
\end{lstlisting}

  ($\Longleftarrow$) Procedemos de manera similar. Sean $C_1$, $C_2$ cerrados (\code{C1\_closed}, \code{C2\_closed}) disjuntos (\code{hC}). Podemos aplicar la hipótesis (\code{h}) al abierto $C_1^c$ y al cerrado $C_2$ para obtener obtener un abierto $V$ (\code{V\_open}) de manera que $C_2 \subseteq V \subseteq \overline{V} \subseteq C_1^c$ (\code{hV}).

\begin{lstlisting}
    intro h C1 C2 C1_closed C2_closed hC
    obtain ⟨V, V_open, hV⟩ :=
      h C1ᶜ C2
      (by exact IsClosed.isOpen_compl)
      C2_closed
      (by rw [← ABdisjoint_iff_AsubsBc, Set.inter_comm C2 C1]; exact hC)
\end{lstlisting}

  Ahora tomamos los abiertos $U_1 = \overline{V}^c$ y $U_2 = V$. Queremos ver que cumplen la condición de normalidad para $C_1$ y $C_2$.
  
\begin{lstlisting}
  IsOpen (Closure V)ᶜ ∧ IsOpen V ∧ C1 ⊆ (Closure V)ᶜ ∧ C2 ⊆ V ∧ (Closure V)ᶜ ∩ V = ∅
\end{lstlisting}
  
  En efecto, ambos son abiertos ($\overline{V}^c$ por ser el complementario de una clausura y $V$ por construcción).

\begin{lstlisting}
    constructor
    · simp
      exact closure_is_closed V
    constructor
    · exact V_open
\end{lstlisting}

  Además, $C_1 \subseteq \overline{V}^c$ es equivalente a $\overline{V} \subseteq C_1^c$, que es cierto por construcción de $V$, igual que $C_2 \subseteq V$.
  
\begin{lstlisting}
    constructor
    · apply Set.subset_compl_comm.mp
      exact hV.right
    constructor
    · exact hV.left
\end{lstlisting}

  Por último, se tiene

  $$
  \overline{V}^c \cap V = \emptyset \iff V \cap \overline{V}^c = \emptyset \iff
  V \subseteq \overline{V}^{cc} \iff V \subseteq \overline{V},
  $$

  que es cierto por las propiedades de la adherencia.

\begin{lstlisting}
    · rw [Set.inter_comm]
      rw [ABdisjoint_iff_AsubsBc]
      simp
      exact set_inside_closure V
\end{lstlisting}



  





  

  




\end{proof}



\section{El Lema de Urysohn}

Introducción:

Definición: espacio normal

Lema (de Urysohn):

Esta demostración es relativamente complicada en papel, pero todavía lo es más cuando intentamos completarla en Lean. Por claridad, dividimos la demostración en varios pasos.

La demostración en Lean empieza utilizando la táctica \bluecode{constructor}, que nos divide el objetivo en dos, uno para cada implicación. Nos centraremos primero en la segunda, que es la más sencilla.

\subsection{El recíproco}





Queremos 

Demostración


\subsection{Numerar los racionales}

Los racionales son denumerables, es decir existe una biyección entre $\nat$ y $\rat$.

En particular, existe una función $f : \nat \to Q$ donde $Q = \rat \cap [0, 1]$, de forma que

\begin{enumerate}
  \item $f$ es biyectiva
  \item $f(0) = 1$
  \item $f(1) = 0$
\end{enumerate}

A partir de ahora llamarmeos $f$ a esta función.

Demostración:

Demostración en Lean: [poner solo las partes importantes?]

\subsection{Una parte}

Sea $n \in \nat$ con $n > 1$. Como $$P_n = \{1, 2, \dots, n-1\}$$ es un conjunto finito y $f$ es sobreyectiva, existen $r$ y $s$ en $P_n$ (es decir, $r < n$ y $s < n$) tales que

\begin{equation} \label{cond_rs}
  f(r) < f(n) < f(s)
\end{equation}

y, además, estas son las mejores elecciones, es decir

\begin{enumerate}
  \item Si $m < n$ es tal que $f(m) < f(n)$, entonces $f(m) \leq f(r)$
  \item Si $m < n$ es tal que $f(n) < f(m)$, entonces $f(s) \leq f(m)$
\end{enumerate}

Además, como $f$ es inyectiva, estas elecciones son únicas.

Para cada $n > 1$, consideramos las funciones $r : \nat \to \nat$ y $s : \nat \to \nat$ que nos dan precisamente estos números $r$ y $n$.

--

Supongamos que hemos encontrado una función $G : \nat \to \mathcal{P}(X)$ que satisface las siguientes propiedades

\begin{enumerate}
  \item $G(0) = X \backslash C_2$
  \item $G(1) = V$ tal que $C_1 \subseteq V \subseteq \overline{V} \subseteq X \backslash C_2$
  \item Para cada $n \in \nat $, $G(n)$ es abierto en $X$
  \item Para cada $n \in \nat$, si $n>1$ entonces \begin{equation} \label{cond_Grs} \overline{G(r(n))} \subseteq G(n) \subseteq \overline{G(n)} \subseteq G(s(n)) \end{equation}
\end{enumerate}

Queremos probar que, en este caso, $G$ también cumple la siguiente propiedad:

\begin{equation} \label{cond_G}
  \forall n, m \in \nat, f(n) < f(m) \implies \overline{G(n)} \subseteq G(m)
\end{equation}


\begin{proof}

Sean $n, m \in \nat$ con $f(n) < f(m)$

Para empezar, como $f(n) < f(m)$, no puede ser $n=0$ ni $m=1$:

\begin{itemize}
  \item Si fuera $n = 0$, entonces $f(n) = 1$ y tendríamos $1 < f(m)$, pero $f$ toma valores en $[0, 1]$.
  \item Si fuera $m = 1$, entonces $f(m) = 0$ y tendríamos $f(n) < 0$, pero $f$ toma valores en $[0, 1]$.
\end{itemize}

Además, $n \neq m$ por ser $f$ inyectiva. Dividimos en los siguientes casos.

\textit{Caso 1.} $n = 1, m = 0$

Trivialmente por hipótesis (prop. 2 de G)

\textit{Caso 2.} $n > 1, m = 0$, por inducción completa sobre $n$

Sea $n>1$ arbitrario y supongamos, por inducción, que para cada $k < n$ con $k > 1$ se tiene $\overline{G(k)} \subseteq G(m)$.

Hay dos posibilidades: o bien $s(n)=0$ o bien $s(n)>1$. En el caso de que $s(n)=0$ es sencillo: tenemos $m=0=s(n)$, por tanto el objetivo pasa a ser $\overline{G(n)} \subseteq G(s(n))$, que es cierto por las propiedades de G.

Supongamos por tanto que $s(n)>1$. Entonces podemos aplicar la hipótesis de inducción a $k=s(n)>1$, y obtenemos que $\overline{G(s(n))} \subseteq G(m)$. Pero, por las propiedades de G,  $\overline{G(n)} \subseteq G(s(n))$. Obtenemos, por las propiedades de la adherencia,

$$\overline{G(n)} \subseteq G(s(n)) \subseteq \overline{G(s(n))} \subseteq G(m)$$

\textit{Caso 3.} $n = 1, m > 1$, por inducción completa sobre $m$

Es simétrico al anterior. Sea $m>1$ arbitrario y supongamos, por inducción, que para cada $l < m$ con $l > 1$ se tiene $\overline{G(n)} \subseteq G(l)$.

Hay dos posibilidades: o bien $r(m)=1$ o bien $r(m)>1$. En el caso de que $r(m)=1$ es sencillo: tenemos $n=1=r(m)$, por tanto el objetivo pasa a ser $\overline{G(r(m))} \subseteq G(m)$, que es cierto por las propiedades de G.

Supongamos por tanto que $r(m)>1$. Entonces podemos aplicar la hipótesis de inducción a $l=r(m)>1$, y obtenemos que $\overline{G(n)} \subseteq G(r(m))$. Pero, por las propiedades de G,  $\overline{G(r(m))} \subseteq G(m)$. Obtenemos, por las propiedades de la adherencia,

$$ \overline{G(n)} \subseteq G(r(m)) \subseteq \overline{G(r(m))} \subseteq G(m) $$

\textit{Caso 4.} $n > 1, m > 1$. Dividimos en dos casos adiccionales:

\textit{Caso 4.1.} $m < n$

Por inducción sobre $n$. Sea $n>m>1$ tal que $f(n) < f(m)$ y supongamos, por inducción, que para cada $k < n$ con $k>m$ y $f(k) < f(m)$ se tiene 

Sabemos que $m < n$ y que $f(n) < f(m)$. Por tanto, podemos aplicar las propiedades de $s$ sobre $n$ ($f(n) < f(s(n))$ es la mejor elección) y obtenemos que $f(s(n)) \leq f(m)$.

Si $f(s(n)) = f(m)$, como $f$ es inyectiva, $s(n)=m$. Luego es trivial porque el objetivo pasa a ser $\overline{G(n)} \subseteq G(s(n))$.

Supongamos que $f(s(n)) < f(m)$.

\textit{Caso 4.2.} $m < n$


Supongamos primero que $n = 1$ y $m = 0$. Entonces conocemos los valores de $G(n)$ y $G(m)$, y la expresión \refpep{cond_G} que queremos probar pasa a ser

$$
  \overline{V} \subseteq X \backslash C_2
$$

que es cierto por hipótesis.

Supongamos ahora que $n, m >1$. Entonces ambas satisfacen \refpep{cond_Grs}. Vamos a probar

\begin{equation} \label{cond_G_inductive}
  \forall n, m > 1, f(n) < f(m) \implies \overline{G(n)} \subseteq G(m)
\end{equation}

Nota:
En un caso extremadamente explicativo (pero para que fuera consistente en cierta manera con la demostración en Lean) quizás podría escribir que esta condición es equivalente a esta otra, sobre la que se puede aplicar inducción completa "normal":

$$
  \forall n, m \in \nat, f(n+2) < f(m+2) \implies \overline{G(n+2)} \subseteq G(m+2)
$$


Por inducción completa sobre $n$ y $m$.

Inducción sobre $n$.

\begin{itemize}
  \item \textit{Caso base} ($n = 2$).
\end{itemize}
  
Como $n=2$, y se tiene $r(n) < n$ y $s(n) < n$, podemos asegurar que $r(n), r(s) \in \{0, 1\}$.

En particular, para que se cumpla \refpep{cond_rs}, necesariamente tiene que ser $$0 = f(1) < f(2) < f(0) = 1$$ y por tanto $r(n) = 1$ y $s(n) = 0.$

Sea entonces un $m > 1$ fijo pero arbitrario. Sabemos que $m \neq 2$ por ser $f$ inyectiva.

Inducción sobre $m$

\begin{itemize}
  \setlength{\itemindent}{2em}
  \item[$\circ$] \textit{Caso base} ($n = 2, m= 3$)
\end{itemize}

Como $f(n) < f(m)$, se tiene $$0 < f(1) < f(2) < f(3) < f(0) = 1.$$ Por tanto, $r(m) = 2 = n$ y $s(m) = 0$. De \refpep{cond_Grs}, se obtiene:

$$
\overline{G(n)} = \overline{G(r(m))} \subseteq G(m),
$$

como queríamos.

\begin{itemize}
  \setlength{\itemindent}{2em}
  \item[$\circ$] \textit{Caso recursivo} ($n = 2, m>3$)
\end{itemize}

Hipótesis de inducción:

\begin{equation} \label{cond_G_ind_m}
  \forall l < m \text{ con } l > 1, f(2) < f(l) \implies \overline{G(2)} \subseteq G(l)
\end{equation}

Veamos primero que se tiene que dar $r(m) > 1$.

\begin{itemize}
  \item $r(m) \neq 0$. Si fuera $r(m) = 0$ tendríamos $1 = f(0) < f(m)$, que es imposible.
  \item $r(m) \neq 1$. Si fuera $r(m) = 1$, como $f(n) < f(m)$ y $r(m)$ es la mejor elección, tendríamos $f(n) \leq f(r(m)) = f(1) = 0$. Como $n=2\neq 1$, entonces $f(n) < 0$, que es imposible.
\end{itemize}


Luego $r(m)> 1$ y podemos aplicar \refpep{cond_G_ind_m} a $l=r(m)<m$, obteniendo $$\overline{G(2)} \subseteq G(r(m)).$$

Por otro lado, por \refpep{cond_Grs}, sabemos que $$\overline{G(r(m))} \subseteq G(m).$$ Mediante las propiedades de la adherencia, unimos las expresiones

$$
\overline{G(2)} \subseteq G(r(m)) \subseteq \overline{G(r(m))} \subseteq G(m),
$$

finalizando por tanto la inducción sobre $m$.

\begin{itemize}
  \item \textit{Caso recursivo} ($n > 2$, $m>1$ es fijo pero arbitrario)
\end{itemize}

Hipótesis de inducción:
\begin{equation} \label{cond_G_ind_n}
  \forall k < n \text{ con } k > 1, f(k) < f(m) \implies \overline{G(k)} \subseteq G(m)
\end{equation}

Tenemos que diferenciar dos casos: $s(n) = 0$ y $s(n) > 1$ (no puede ser $s(n) = 1$ porque en ese caso tendríamos $f(n) < f(s(n)) = f(1) = 0$, que es imposible).

Supongamos primero que $s(n) = 0$. Entonces, como $f(n) < f(m)$ y $s(n)$ es la mejor elección, tenemos que $f(s(n)) \leq f(m)$. Sin embargo, en realidad no puede darse $f(s(n)) < f(m)$, porque tendríamos $1 = f(0) = f(s(n)) < f(m)$, que es imposible. Luego en caso de que $s(n)=0$, necesariamente se tiene $f(s(n)) = f(m)$, y por inyectividad de $f$, $s(n) = m$. Entonces el resultado se sigue trivialmente de \refpep{cond_Grs} aplicado a $n>1$:

$$
  \overline{G(n)} \subseteq G(s(n)) = G(m).
$$

Si suponemos ahora que $s(n)>1$, podemos aplicar \refpep{cond_G_ind_n} a $k = s(n) < n$, obteniendo $$\overline{G(s(n))} \subseteq G(m).$$

Además, por \refpep{cond_Grs}, como $n > 1$, se tiene $$\overline{G(n)} \subseteq G(s(n)),$$ y, por las propiedades de la adherencia, podemos unir ambas expresiones $$\overline{G(n)} \subseteq G(s(n)) \subseteq \overline{G(s(n))} \subseteq G(m)$$ obteniendo el resultado que buscábamos.


\end{proof}

% Bibliografía

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}
